@article{Barate1999,
author = {Barate, R. and Decamp, D. and Ghez, et al.
%P. and Goy, C. and Jezequel, S. and Lees, J.-P. and Martin, F. and Merle, E. and Minard, M.-N. and Pietrzyk, B and Alemany, R. and Bravo, S. and Casado, M.P. and Chmeissani, M. and Crespo, J.M. and Fernandez, E. and Fernandez-Bosman, M. and Garrido, Ll. and Graug{\`{e}}s, E. and Juste, A. and Martinez, M. and Merino, G. and Miquel, R. and Mir, Ll.M. and Morawitz, P. and Pacheco, A. and Riu, I. and Ruiz, H. and Colaleo, A. and Creanza, D. and de Palma, M. and Iaselli, G. and Maggi, G. and Maggi, M. and Nuzzo, S. and Ranieri, A. and Raso, G. and Ruggieri, F. and Selvaggi, G. and Silvestris, L. and Tempesta, P. and Tricomi, A. and Zito, G. and Huang, X. and Lin, J. and Ouyang, Q. and Wang, T. and Xie, Y. and Xu, R. and Xue, S. and Zhang, J. and Zhang, L. and Zhao, W. and Abbaneo, D. and Boix, G. and Buchm{\"{u}}ller, O. and Cattaneo, M. and Cerutti, F. and Ciulli, V. and Davies, G. and Dissertori, G. and Drevermann, H. and Forty, R.W. and Frank, M. and Gianotti, F. and Greening, T.C. and Halley, A.W. and Hansen, J.B. and Harvey, J. and Janot, P. and Jost, B. and Lehraus, I. and Leroy, O. and Maley, P. and Mato, P. and Minten, A. and Moutoussi, A. and Ranjard, F. and Rolandi, L. and Schlatter, D. and Schmitt, M. and Schneider, O. and Spagnolo, P. and Tejessy, W. and Teubert, F. and Tournefier, E. and Wright, A.E. and Ajaltouni, Z. and Badaud, F. and Chazelle, G. and Deschamps, O. and Dessagne, S. and Falvard, A. and Ferdi, C. and Gay, P. and Guicheney, C. and Henrard, P. and Jousset, J. and Michel, B. and Monteil, S. and Montret, J-C. and Pallin, D. and Perret, P. and Podlyski, F. and Hansen, J.D. and Hansen, J.R. and Hansen, P.H. and Nilsson, B.S. and Rensch, B. and W{\"{a}}{\"{a}}n{\"{a}}nen, A. and Daskalakis, G. and Kyriakis, A. and Markou, C. and Simopoulou, E. and Vayaki, A. and Blondel, A. and Brient, J.-C. and Machefert, F. and Roug{\'{e}}, A. and Swynghedauw, M. and Tanaka, R. and Valassi, A. and Videau, H. and Focardi, E. and Parrini, G. and Zachariadou, K. and Corden, M. and Georgiopoulos, C. and Antonelli, A. and Bencivenni, G. and Bologna, G. and Bossi, F. and Campana, P. and Capon, G. and Chiarella, V. and Laurelli, P. and Mannocchi, G. and Murtas, F. and Murtas, G.P. and Passalacqua, L. and Pepe-Altarelli, M. and Chalmers, M. and Curtis, L. and Lynch, J.G. and Negus, P. and O'Shea, V. and Raeven, B. and Raine, C. and Smith, D. and Teixeira-Dias, P. and Thompson, A.S. and Ward, J.J. and Cavanaugh, R. and Dhamotharan, S. and Geweniger, C. and Hanke, P. and Hepp, V. and Kluge, E.E. and Putzer, A. and Tittel, K. and Werner, S. and Wunsch, M. and Beuselinck, R. and Binnie, D.M. and Cameron, W. and Dornan, P.J. and Girone, M. and Goodsir, S. and Marinelli, N. and Martin, E.B. and Nash, J. and Nowell, J. and Przysiezniak, H. and Sciab{\`{a}}, A. and Sedgbeer, J.K. and Thomson, E. and Williams, M.D. and Ghete, V.M. and Girtler, P. and Kneringer, E. and Kuhn, D. and Rudolph, G. and Bowdery, C.K. and Buck, P.G. and Ellis, G. and Finch, A.J. and Foster, F. and Hughes, G. and Jones, R.W.L. and Robertson, N.A. and Smizanska, M. and Williams, M.I. and Giehl, I. and H{\"{o}}lldorfer, F. and Jakobs, K. and Kleinknecht, K. and Kr{\"{o}}cker, M. and M{\"{u}}ller, A.-S. and N{\"{u}}rnberger, H.-A. and Quast, G. and Renk, B. and Rohne, E. and Sander, H.-G. and Schmeling, S. and Wachsmuth, H. and Zeitnitz, C. and Ziegler, T. and Aubert, J.J. and Bonissent, A. and Carr, J. and Coyle, P. and Ealet, A. and Fouchez, D. and Tilquin, A. and Aleppo, M. and Antonelli, M. and Gilardoni, S. and Ragusa, F. and B{\"{u}}scher, V. and Dietl, H. and Ganis, G. and H{\"{u}}ttmann, K. and L{\"{u}}tjens, G. and Mannert, C. and M{\"{a}}nner, W. and Moser, H.-G. and Schael, S. and Settles, R. and Seywerd, H. and Stenzel, H. and Wiedenmann, W. and Wolf, G. and Azzurri, P. and Boucrot, J. and Callot, O. and Chen, S. and Davier, M. and Duflot, L. and Grivaz, J.-F. and Heusse, Ph. and Jacholkowska, A. and Kado, M. and Lefran{\c{c}}ois, J. and Serin, L. and Veillet, J.-J. and Videau, I. and {de Vivie de R{\'{e}}gie}, J.-B. and Zerwas, D. and Bagliesi, G. and Boccali, T. and Bozzi, C. and Calderini, G. and Dell'Orso, R. and Ferrante, I. and Giassi, A. and Gregorio, A. and Ligabue, F. and Marrocchesi, P.S. and Messineo, A. and Palla, F. and Rizzo, G. and Sanguinetti, G. and Sguazzoni, G. and Tenchini, R. and Venturi, A. and Verdini, P.G. and Blair, G.A. and Coles, J. and Cowan, G. and Green, M.G. and Hutchcroft, D.E. and Jones, L.T. and Medcalf, T. and Strong, J.A. and Botterill, D.R. and Clifft, R.W. and Edgecock, T.R. and Norton, P.R. and Thompson, J.C. and Tomalin, I.R. and Bloch-Devaux, B. and Colas, P. and Fabbro, B. and Fa{\"{i}}f, G. and Lan{\c{c}}on, E. and Lemaire, M.-C. and Locci, E. and Perez, P. and Rander, J. and Renardy, J.-F. and Rosowsky, A. and Seager, P. and Trabelsi, A. and Tuchming, B. and Vallage, B. and Black, S.N. and Dann, J.H. and Loomis, C. and Kim, H.Y. and Konstantinidis, N. and Litke, A.M. and McNeil, M.A. and Taylor, G. and Booth, C.N. and Cartwright, S. and Combley, F. and Hodgson, P.N. and Lehto, M. and Thompson, L.F. and Affholderbach, K. and B{\"{o}}hrer, A. and Brandt, S. and Grupen, C. and Hess, J. and Misiejuk, A. and Prange, G. and Sieler, U. and Giannini, G. and Gobbo, B. and Putz, J. and Rothberg, J. and Wasserbaech, S. and Williams, R.W. and Armstrong, S.R. and Elmer, P. and Ferguson, D.P.S. and Gao, Y. and Gonz{\'{a}}lez, S. and Hayes, O.J. and Hu, H. and Jin, S. and Kile, J. and {McNamara III}, P.A. and Nielsen, J. and Orejudos, W. and Pan, Y.B. and Saadi, Y. and Scott, I.J. and Walsh, J. and von Wimmersperg-Toeller, J.H. and Wu, Sau Lan and Wu, X. and Zobernig, G.
},
doi = {10.1016/S0370-2693(99)01288-5},
file = {:Users/retico/cernbox/Documents/Literature/MonteCarlo-capitolo-libro/HEP/1-s2.0-S0370269399012885-main.pdf:pdf},
issn = {03702693},
journal = {Physics Letters B},
month = {dec},
number = {1-4},
pages = {287--302},
title = {{Measurement of the e+e−→ZZ production cross section at centre-of-mass energies of 183 and 189 GeV}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0370269399012885},
volume = {469},
year = {1999}
}

@article{Sahiner1996,
abstract = {We investigated the classification of regions of interest (ROI's) on mammograms as either mass or normal tissue using a convolution neural network (CNN). A CNN is a backpropagation neural network with two-dimensional (2-D) weight kernels that operate on images. A generalized, fast and stable implementation of the CNN was developed. The input images to the CNN were obtained from the ROI's using two techniques. The first technique employed averaging and subsampling. The second technique employed texture feature extraction methods applied to small subregions inside the ROI. Features computed over different subregions were arranged as texture images, which were subsequently used as CNN inputs. The effects of CNN architecture and texture feature parameters on classification accuracy were studied. Receiver operating characteristic (ROC) methodology was used to evaluate the classification accuracy. A data set consisting of 168 ROI's containing biopsy-proven masses and 504 ROI's containing normal breast tissue was extracted from 168 mammograms by radiologists experienced in mammography. This data set was used for training and testing the CNN. With the best combination of CNN architecture and texture feature parameters, the area under the test ROC curve reached 0.87, which corresponded to a true-positive fraction of 90% at a false positive fraction of 31%. Our results demonstrate the feasibility of using a CNN for classification of masses and normal tissue on mammograms. {\textcopyright} 1996 IEEE.},
author = {Sahiner, Berkman and Chan, Heang Ping and Petrick, Nicholas and Wei, Datong and Helvie, Mark A. and Adler, Dorit D. and Goodsitt, Mitchell M.},
doi = {10.1109/42.538937},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
number = {5},
pages = {598--610},
pmid = {18215941},
title = {{Classification of mass and normal breast tissue: A convolution neural network classifier with spatial domain and texture images}},
volume = {15},
year = {1996}
}

@article{Ben-Bassat1980,
abstract = {The sensitivity of Bayesian pattern recognition models to multiplicative deviations in the prior and conditional probabilities is investigated for the two-class case. Explicit formulas are obtained for the factor K by which the computed posterior probabilities should be divided in order to eliminate the deviation effect. Numerical results for the case of binary features indicate that the Bayesian model tolerates large deviations in the prior and conditional probabilities. In fact, the a priori ratio and the likelihood ratio may deviate within a range of 65-135 percent and still produce posterior probabilities in accurate proximity of at most ±0.10.The main implication is that Bayesian systems which are based on limited data or subjective probabilities are expected to have a high percentage of correct classification despite the fact that the prior and conditional probabilities they use may deviate rather significantly from the true values. Copyright {\textcopyright} 1980 by The Institute of Electrical and Electronics Engineers. Inc.},
author = {Ben-Bassat, Moshe and Klove, Karin L. and Weil, Max H.},
doi = {10.1109/TPAMI.1980.4767015},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {261--266},
title = {{Sensitivity analysis in bayesian classification models: Multiplicative deviations}},
volume = {PAMI-2},
year = {1980}
}


%% ml intro

@misc{neuronlength,
title = {{Transmitting fibers in the brain : Total length and distribution of lengths - AI IMPACTS}},
url = {https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/},
urldate = {2021-04-29}
}



@article{Hodgkin1952,
author = {Hodgkin, A. L. and Huxley, A. F.},
doi = {10.1113/jphysiol.1952.sp004764},
file = {::},
issn = {0022-3751},
journal = {The Journal of Physiology},
month = {aug},
number = {4},
pages = {500--544},
publisher = {John Wiley & Sons, Ltd},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
volume = {117},
year = {1952}
}


@book{Gerstner2002,
author = {Gerstner, Wulfram},
booktitle = {Cambridge University Press},
title = {{Spiking neuron models : single neurons, populations, plasticity}},
year = {2002}
}



@misc{neuronWiki,
title = {{Artificial neuron - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Artificial_neuron},
urldate = {2021-04-30}
}


@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp($\mu$) performance criteria, for arbitrary finite input environment measures $\mu$, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. {\textcopyright} 1991.},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp($\mu$) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
month = {jan},
number = {2},
pages = {251--257},
publisher = {Pergamon},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}


@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATION, STORAGE, MODEL FOR, IN BRAIN BRAIN, INFORMATION STORAGE IN, MODEL FOR LEARNING & MEMORY},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
url = {/record/1959-09865-001},
volume = {65},
year = {1958}
}

@misc{logistic,
title = {{Logistic function - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Logistic_function},
urldate = {2021-04-30}
}

@article{Wang2020,
abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
doi = {10.1007/s40745-020-00253-5},
file = {::},
issn = {21985812},
journal = {Annals of Data Science},
keywords = {Deep learning,Loss function,Machine learning,Survey},
month = {apr},
pages = {1--26},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{A Comprehensive Survey of Loss Functions in Machine Learning}},
url = {https://doi.org/10.1007/s40745-020-00253-5},
year = {2020}
}


@misc{Anwar2018,
abstract = {The science of solving clinical problems by analyzing images generated in clinical practice is known as medical image analysis. The aim is to extract information in an affective and efficient manner for improved clinical diagnosis. The recent advances in the field of biomedical engineering have made medical image analysis one of the top research and development area. One of the reasons for this advancement is the application of machine learning techniques for the analysis of medical images. Deep learning is successfully used as a tool for machine learning, where a neural network is capable of automatically learning features. This is in contrast to those methods where traditionally hand crafted features are used. The selection and calculation of these features is a challenging task. Among deep learning techniques, deep convolutional networks are actively used for the purpose of medical image analysis. This includes application areas such as segmentation, abnormality detection, disease classification, computer aided diagnosis and retrieval. In this study, a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks is presented. The challenges and potential of these techniques are also highlighted.},
archivePrefix = {arXiv},
arxivId = {1709.02250},
author = {Anwar, Syed Muhammad and Majid, Muhammad and Qayyum, Adnan and Awais, Muhammad and Alnowami, Majdi and Khan, Muhammad Khurram},
booktitle = {Journal of Medical Systems},
doi = {10.1007/s10916-018-1088-1},
eprint = {1709.02250},
file = {::},
issn = {1573689X},
keywords = {Classification,Computer aided diagnosis,Convolutional neural network,Medical image analysis,Segmentation},
month = {nov},
number = {11},
pages = {226},
pmid = {30298337},
publisher = {Springer New York LLC},
title = {{Medical Image Analysis using Convolutional Neural Networks: A Review}},
url = {https://doi.org/10.1007/s10916-018-1088-1},
volume = {42},
year = {2018}
}

@article{Liu2021,
abstract = {Scientific simulations on high-performance computing (HPC) systems can generate large amounts of floating-point data per run. As compared to lossless compressors, lossy compressors, such as SZ and ZFP, can reduce data volume more aggressively while maintaining the usefulness of the data. However, a reduction ratio of more than two orders of magnitude is almost impossible without seriously distorting the data. In deep learning, the autoencoder has shown potential for data compression. Whether the autoencoder can deliver similar performance on scientific data, however, is unknown. In this paper, we for the first time conduct a comprehensive study on the use of autoencoders to compress real-world scientific data and illustrate several key findings on using autoencoders for scientific data reduction. We implement an autoencoder-based compression prototype to reduce floating-point data. Our study shows that the out-of-the-box implementation needs to be further tuned in order to achieve high compression ratios and satisfactory error bounds. Our evaluation results show that, for most test datasets, the tuned autoencoder outperforms SZ by 2 to 4X, and ZFP by 10 to 50X in compression ratios, respectively. Our practices and lessons learned in this work can direct future optimizations for using autoencoders to compress scientific data.},
author = {Liu, Tong and Wang, Jinzhen and Liu, Qing and Alibhai, Shakeel and Lu, Tao and He, Xubin},
doi = {10.1109/TBDATA.2021.3066151},
issn = {23327790},
journal = {IEEE Transactions on Big Data},
keywords = {Big Data,Compressors,Data models,Decoding,Image coding,Lossy data compression,Prototypes,Tuning,autoencoder,machine learning,scientific data},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{High-Ratio Lossy Compression: Exploring the Autoencoder to Compress Scientific Data}},
year = {2021}
}

@article{Liu2021,
abstract = {Scientific simulations on high-performance computing (HPC) systems can generate large amounts of floating-point data per run. As compared to lossless compressors, lossy compressors, such as SZ and ZFP, can reduce data volume more aggressively while maintaining the usefulness of the data. However, a reduction ratio of more than two orders of magnitude is almost impossible without seriously distorting the data. In deep learning, the autoencoder has shown potential for data compression. Whether the autoencoder can deliver similar performance on scientific data, however, is unknown. In this paper, we for the first time conduct a comprehensive study on the use of autoencoders to compress real-world scientific data and illustrate several key findings on using autoencoders for scientific data reduction. We implement an autoencoder-based compression prototype to reduce floating-point data. Our study shows that the out-of-the-box implementation needs to be further tuned in order to achieve high compression ratios and satisfactory error bounds. Our evaluation results show that, for most test datasets, the tuned autoencoder outperforms SZ by 2 to 4X, and ZFP by 10 to 50X in compression ratios, respectively. Our practices and lessons learned in this work can direct future optimizations for using autoencoders to compress scientific data.},
author = {Liu, Tong and Wang, Jinzhen and Liu, Qing and Alibhai, Shakeel and Lu, Tao and He, Xubin},
doi = {10.1109/TBDATA.2021.3066151},
issn = {23327790},
journal = {IEEE Transactions on Big Data},
keywords = {Big Data,Compressors,Data models,Decoding,Image coding,Lossy data compression,Prototypes,Tuning,autoencoder,machine learning,scientific data},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{High-Ratio Lossy Compression: Exploring the Autoencoder to Compress Scientific Data}},
year = {2021}
}

@article{Knapp2021,
abstract = {We apply an Adversarially Learned Anomaly Detection (ALAD) algorithm to the problem of detecting new physics processes in proton-proton collisions at the Large Hadron Collider. Anomaly detection based on ALAD matches performances reached by Variational Autoencoders, with a substantial improvement in some cases. Training the ALAD algorithm on 4.4 fb −1 of 8 TeV CMS Open Data, we show how a data-driven anomaly detection and characterization would work in real life, rediscovering the top quark by identifying the main features of the t ¯ t experimental signature at the LHC.},
author = {Knapp, O and Cerri, O and Dissertori, G and Nguyen, T Q and Pierini, M and Vlimant, J R},
doi = {10.1140/epjp/s13360-021-01109-4},
file = {::},
isbn = {0123456789},
journal = {Eur. Phys. J. Plus},
pages = {236},
title = {{Adversarially Learned Anomaly Detection on CMS open data: re-discovering the top quark}},
url = {https://doi.org/10.1140/epjp/s13360-021-01109-4},
volume = {136},
year = {2021}
}


@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-encoding variational bayes}},
url = {https://arxiv.org/abs/1312.6114v10},
year = {2014}
}


@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
file = {::},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-encoding variational bayes}},
url = {https://arxiv.org/abs/1312.6114v10},
year = {2014}
}

@article{Kullback1951,
abstract = {Volume 22, Number 1 (1951), 1-164},
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177729694},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {mar},
number = {1},
pages = {79--86},
publisher = {Institute of Mathematical Statistics},
title = {{On Information and Sufficiency}},
url = {https://www.scienceopen.com/document?vid=3e5c5966-a6a6-4026-99c8-bf9861cb15bb},
volume = {22},
year = {1951}
}






















@article{Sarrut2019,
author = {Sarrut, D and Krah, N and L{\'{e}}tang, J M},
doi = {10.1088/1361-6560/ab3fc1},
file = {:Users/retico/cernbox/Documents/Literature/MonteCarlo/Sarrut et al. - 2019 - Generative adversarial networks (GAN) for compact -MonteCarlo.pdf:pdf},
issn = {1361-6560},
journal = {Physics in Medicine & Biology},
keywords = {generative adversarial network,linac,monte-carlo simulation,phase-space},
month = {oct},
number = {21},
pages = {215004},
title = {{Generative adversarial networks (GAN) for compact beam source modelling in Monte Carlo simulations}},
url = {https://iopscience.iop.org/article/10.1088/1361-6560/ab3fc1},
volume = {64},
year = {2019}
}
@article{Sarrut2018,
abstract = {A method to speed up Monte-Carlo simulations of single photon emission computed tomography (SPECT) imaging is proposed. It uses an artificial neural network (ANN) to learn the angular response function (ARF) of a collimatordetector system. The ANN is trained once from a complete simulation including the complete detector head with collimator, crystal, and digitization process. In the simulation, particle tracking inside the SPECT head is replaced by a plane. Photons are stopped at the plane and the energy and direction are used as input to the ANN, which provides detection probabilities in each energy window. Compared to histogram-based ARF, the proposed method is less dependent on the statistics of the training data, provides similar simulation efficiency, and requires less training data. The implementation is available within the GATE platform.},
author = {Sarrut, D. and Krah, N. and Badel, J. N. and L{\'{e}}tang, J. M.},
doi = {10.1088/1361-6560/aae331},
file = {:Users/retico/cernbox/Documents/Literature/MonteCarlo/Sarrut_2018_Phys._Med._Biol._63_205013.pdf:pdf},
issn = {13616560},
journal = {Physics in Medicine and Biology},
keywords = {Monte-Carlo simulation,SPECT imaging,neural network,variance reduction technique},
number = {20},
pmid = {30238925},
title = {{Learning SPECT detector angular response function with neural network for accelerating Monte-Carlo simulations}},
volume = {63},
year = {2018}
}
