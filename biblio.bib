@article{Barate1999,
author = {Barate, R. and Decamp, D. and Ghez et al., P.},
doi = {10.1016/S0370-2693(99)01288-5},
file = {:Users/retico/cernbox/Documents/Literature/MonteCarlo-capitolo-libro/HEP/1-s2.0-S0370269399012885-main.pdf:pdf},
issn = {03702693},
journal = {Physics Letters B},
month = {dec},
number = {1-4},
pages = {287--302},
title = {{Measurement of the e+e−→ZZ production cross section at centre-of-mass energies of 183 and 189 GeV}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0370269399012885},
volume = {469},
year = {1999}
}

@article{Sahiner1996,
abstract = {We investigated the classification of regions of interest (ROI's) on mammograms as either mass or normal tissue using a convolution neural network (CNN). A CNN is a backpropagation neural network with two-dimensional (2-D) weight kernels that operate on images. A generalized, fast and stable implementation of the CNN was developed. The input images to the CNN were obtained from the ROI's using two techniques. The first technique employed averaging and subsampling. The second technique employed texture feature extraction methods applied to small subregions inside the ROI. Features computed over different subregions were arranged as texture images, which were subsequently used as CNN inputs. The effects of CNN architecture and texture feature parameters on classification accuracy were studied. Receiver operating characteristic (ROC) methodology was used to evaluate the classification accuracy. A data set consisting of 168 ROI's containing biopsy-proven masses and 504 ROI's containing normal breast tissue was extracted from 168 mammograms by radiologists experienced in mammography. This data set was used for training and testing the CNN. With the best combination of CNN architecture and texture feature parameters, the area under the test ROC curve reached 0.87, which corresponded to a true-positive fraction of 90% at a false positive fraction of 31%. Our results demonstrate the feasibility of using a CNN for classification of masses and normal tissue on mammograms. {\textcopyright} 1996 IEEE.},
author = {Sahiner, Berkman and Chan, Heang Ping and Petrick, Nicholas and Wei, Datong and Helvie, Mark A. and Adler, Dorit D. and Goodsitt, Mitchell M.},
doi = {10.1109/42.538937},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
number = {5},
pages = {598--610},
pmid = {18215941},
title = {{Classification of mass and normal breast tissue: A convolution neural network classifier with spatial domain and texture images}},
volume = {15},
year = {1996}
}

@article{Ben-Bassat1980,
abstract = {The sensitivity of Bayesian pattern recognition models to multiplicative deviations in the prior and conditional probabilities is investigated for the two-class case. Explicit formulas are obtained for the factor K by which the computed posterior probabilities should be divided in order to eliminate the deviation effect. Numerical results for the case of binary features indicate that the Bayesian model tolerates large deviations in the prior and conditional probabilities. In fact, the a priori ratio and the likelihood ratio may deviate within a range of 65-135 percent and still produce posterior probabilities in accurate proximity of at most ±0.10.The main implication is that Bayesian systems which are based on limited data or subjective probabilities are expected to have a high percentage of correct classification despite the fact that the prior and conditional probabilities they use may deviate rather significantly from the true values. Copyright {\textcopyright} 1980 by The Institute of Electrical and Electronics Engineers. Inc.},
author = {Ben-Bassat, Moshe and Klove, Karin L. and Weil, Max H.},
doi = {10.1109/TPAMI.1980.4767015},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {3},
pages = {261--266},
title = {{Sensitivity analysis in bayesian classification models: Multiplicative deviations}},
volume = {PAMI-2},
year = {1980}
}


%% ml intro

@misc{neuronlength,
title = {{Transmitting fibers in the brain : Total length and distribution of lengths - AI IMPACTS}},
url = {https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/},
urldate = {2021-04-29}
}



@article{Hodgkin1952,
author = {Hodgkin, A. L. and Huxley, A. F.},
doi = {10.1113/jphysiol.1952.sp004764},
file = {::},
issn = {0022-3751},
journal = {The Journal of Physiology},
month = {aug},
number = {4},
pages = {500--544},
publisher = {John Wiley & Sons, Ltd},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
volume = {117},
year = {1952}
}


@book{Gerstner2002,
author = {Gerstner, Wulfram},
booktitle = {Cambridge University Press},
title = {{Spiking neuron models : single neurons, populations, plasticity}},
year = {2002}
}



@misc{neuronWiki,
title = {{Artificial neuron - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Artificial_neuron},
urldate = {2021-04-30}
}


@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp($\mu$) performance criteria, for arbitrary finite input environment measures $\mu$, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. {\textcopyright} 1991.},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp($\mu$) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
month = {jan},
number = {2},
pages = {251--257},
publisher = {Pergamon},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}


@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
issn = {0033295X},
journal = {Psychological Review},
keywords = {PERCEPTION, AS INFORMATION STORAGE MODEL INFORMATION, STORAGE, MODEL FOR, IN BRAIN BRAIN, INFORMATION STORAGE IN, MODEL FOR LEARNING & MEMORY},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
url = {/record/1959-09865-001},
volume = {65},
year = {1958}
}

@misc{logistic,
title = {{Logistic function - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Logistic_function},
urldate = {2021-04-30}
}

@article{Wang2020,
abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
doi = {10.1007/s40745-020-00253-5},
file = {::},
issn = {21985812},
journal = {Annals of Data Science},
keywords = {Deep learning,Loss function,Machine learning,Survey},
month = {apr},
pages = {1--26},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{A Comprehensive Survey of Loss Functions in Machine Learning}},
url = {https://doi.org/10.1007/s40745-020-00253-5},
year = {2020}
}


@misc{Anwar2018,
abstract = {The science of solving clinical problems by analyzing images generated in clinical practice is known as medical image analysis. The aim is to extract information in an affective and efficient manner for improved clinical diagnosis. The recent advances in the field of biomedical engineering have made medical image analysis one of the top research and development area. One of the reasons for this advancement is the application of machine learning techniques for the analysis of medical images. Deep learning is successfully used as a tool for machine learning, where a neural network is capable of automatically learning features. This is in contrast to those methods where traditionally hand crafted features are used. The selection and calculation of these features is a challenging task. Among deep learning techniques, deep convolutional networks are actively used for the purpose of medical image analysis. This includes application areas such as segmentation, abnormality detection, disease classification, computer aided diagnosis and retrieval. In this study, a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks is presented. The challenges and potential of these techniques are also highlighted.},
archivePrefix = {arXiv},
arxivId = {1709.02250},
author = {Anwar, Syed Muhammad and Majid, Muhammad and Qayyum, Adnan and Awais, Muhammad and Alnowami, Majdi and Khan, Muhammad Khurram},
booktitle = {Journal of Medical Systems},
doi = {10.1007/s10916-018-1088-1},
eprint = {1709.02250},
file = {::},
issn = {1573689X},
keywords = {Classification,Computer aided diagnosis,Convolutional neural network,Medical image analysis,Segmentation},
month = {nov},
number = {11},
pages = {226},
pmid = {30298337},
publisher = {Springer New York LLC},
title = {{Medical Image Analysis using Convolutional Neural Networks: A Review}},
url = {https://doi.org/10.1007/s10916-018-1088-1},
volume = {42},
year = {2018}
}

@article{Liu2021,
abstract = {Scientific simulations on high-performance computing (HPC) systems can generate large amounts of floating-point data per run. As compared to lossless compressors, lossy compressors, such as SZ and ZFP, can reduce data volume more aggressively while maintaining the usefulness of the data. However, a reduction ratio of more than two orders of magnitude is almost impossible without seriously distorting the data. In deep learning, the autoencoder has shown potential for data compression. Whether the autoencoder can deliver similar performance on scientific data, however, is unknown. In this paper, we for the first time conduct a comprehensive study on the use of autoencoders to compress real-world scientific data and illustrate several key findings on using autoencoders for scientific data reduction. We implement an autoencoder-based compression prototype to reduce floating-point data. Our study shows that the out-of-the-box implementation needs to be further tuned in order to achieve high compression ratios and satisfactory error bounds. Our evaluation results show that, for most test datasets, the tuned autoencoder outperforms SZ by 2 to 4X, and ZFP by 10 to 50X in compression ratios, respectively. Our practices and lessons learned in this work can direct future optimizations for using autoencoders to compress scientific data.},
author = {Liu, Tong and Wang, Jinzhen and Liu, Qing and Alibhai, Shakeel and Lu, Tao and He, Xubin},
doi = {10.1109/TBDATA.2021.3066151},
issn = {23327790},
journal = {IEEE Transactions on Big Data},
keywords = {Big Data,Compressors,Data models,Decoding,Image coding,Lossy data compression,Prototypes,Tuning,autoencoder,machine learning,scientific data},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{High-Ratio Lossy Compression: Exploring the Autoencoder to Compress Scientific Data}},
year = {2021}
}

@article{Liu2021,
abstract = {Scientific simulations on high-performance computing (HPC) systems can generate large amounts of floating-point data per run. As compared to lossless compressors, lossy compressors, such as SZ and ZFP, can reduce data volume more aggressively while maintaining the usefulness of the data. However, a reduction ratio of more than two orders of magnitude is almost impossible without seriously distorting the data. In deep learning, the autoencoder has shown potential for data compression. Whether the autoencoder can deliver similar performance on scientific data, however, is unknown. In this paper, we for the first time conduct a comprehensive study on the use of autoencoders to compress real-world scientific data and illustrate several key findings on using autoencoders for scientific data reduction. We implement an autoencoder-based compression prototype to reduce floating-point data. Our study shows that the out-of-the-box implementation needs to be further tuned in order to achieve high compression ratios and satisfactory error bounds. Our evaluation results show that, for most test datasets, the tuned autoencoder outperforms SZ by 2 to 4X, and ZFP by 10 to 50X in compression ratios, respectively. Our practices and lessons learned in this work can direct future optimizations for using autoencoders to compress scientific data.},
author = {Liu, Tong and Wang, Jinzhen and Liu, Qing and Alibhai, Shakeel and Lu, Tao and He, Xubin},
doi = {10.1109/TBDATA.2021.3066151},
issn = {23327790},
journal = {IEEE Transactions on Big Data},
keywords = {Big Data,Compressors,Data models,Decoding,Image coding,Lossy data compression,Prototypes,Tuning,autoencoder,machine learning,scientific data},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{High-Ratio Lossy Compression: Exploring the Autoencoder to Compress Scientific Data}},
year = {2021}
}

@article{Knapp2021,
abstract = {We apply an Adversarially Learned Anomaly Detection (ALAD) algorithm to the problem of detecting new physics processes in proton-proton collisions at the Large Hadron Collider. Anomaly detection based on ALAD matches performances reached by Variational Autoencoders, with a substantial improvement in some cases. Training the ALAD algorithm on 4.4 fb −1 of 8 TeV CMS Open Data, we show how a data-driven anomaly detection and characterization would work in real life, rediscovering the top quark by identifying the main features of the t ¯ t experimental signature at the LHC.},
author = {Knapp, O and Cerri, O and Dissertori, G and Nguyen, T Q and Pierini, M and Vlimant, J R},
doi = {10.1140/epjp/s13360-021-01109-4},
file = {::},
isbn = {0123456789},
journal = {Eur. Phys. J. Plus},
pages = {236},
title = {{Adversarially Learned Anomaly Detection on CMS open data: re-discovering the top quark}},
url = {https://doi.org/10.1140/epjp/s13360-021-01109-4},
volume = {136},
year = {2021}
}


@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-encoding variational bayes}},
url = {https://arxiv.org/abs/1312.6114v10},
year = {2014}
}


@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P. and Welling, Max},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
eprint = {1312.6114},
file = {::},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Auto-encoding variational bayes}},
url = {https://arxiv.org/abs/1312.6114v10},
year = {2014}
}

@article{Kullback1951,
abstract = {Volume 22, Number 1 (1951), 1-164},
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177729694},
file = {::},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {mar},
number = {1},
pages = {79--86},
publisher = {Institute of Mathematical Statistics},
title = {{On Information and Sufficiency}},
url = {https://www.scienceopen.com/document?vid=3e5c5966-a6a6-4026-99c8-bf9861cb15bb},
volume = {22},
year = {1951}
}

@article{Kingma2019,
abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
archivePrefix = {arXiv},
arxivId = {1906.02691},
author = {Kingma, Diederik P. and Welling, Max},
doi = {10.1561/2200000056},
eprint = {1906.02691},
file = {:Users/retico/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - 2019 - An introduction to variational autoencoders.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
keywords = {Machine Learning},
month = {nov},
number = {4},
pages = {307--392},
publisher = {Now Publishers Inc},
title = {{An introduction to variational autoencoders}},
url = {http://dx.doi.org/10.1561/2200000056},
volume = {12},
year = {2019}
}

@article{Goodfellow2020,
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1145/3422622},
eprint = {1406.2661},
file = {::},
issn = {15577317},
journal = {Communications of the ACM},
month = {oct},
number = {11},
pages = {139--144},
publisher = {Association for Computing Machinery},
title = {{Generative adversarial networks}},
url = {http://www.github.com/goodfeli/adversarial},
volume = {63},
year = {2020}
}

@misc{faces,
title = {{This Person Does Not Exist}},
url = {https://thispersondoesnotexist.com/},
urldate = {2021-04-30}
}

@article{Paganini2018,
abstract = {The precise modeling of subatomic particle interactions and propagation through matter is paramount for the advancement of nuclear and particle physics searches and precision measurements. The most computationally expensive step in the simulation pipeline of a typical experiment at the Large Hadron Collider (LHC) is the detailed modeling of the full complexity of physics processes that govern the motion and evolution of particle showers inside calorimeters. We introduce CALOGAN, a new fast simulation technique based on generative adversarial networks (GANs). We apply these neural networks to the modeling of electromagnetic showers in a longitudinally segmented calorimeter and achieve speedup factors comparable to or better than existing full simulation techniques on CPU (100×-1000×) and even faster on GPU (up to ∼10 5 ×). There are still challenges for achieving precision across the entire phase space, but our solution can reproduce a variety of geometric shower shape properties of photons, positrons, and charged pions. This represents a significant stepping stone toward a full neural network-based detector simulation that could save significant computing time and enable many analyses now and in the future.},
author = {Paganini, Michela and {De Oliveira}, Luke and Nachman, Benjamin},
doi = {10.1103/PhysRevD.97.014021},
keywords = {doi:10.1103/PhysRevD.97.014021 url:https://doi.org/10.1103/PhysRevD.97.014021},
title = {{CALOGAN: Simulating 3D high energy particle showers in multilayer electromagnetic calorimeters with generative adversarial networks}},
year = {2018}
}


@article{Sarrut2019,
author = {Sarrut, D and Krah, N and L{\'{e}}tang, J M},
doi = {10.1088/1361-6560/ab3fc1},
file = {:Users/retico/cernbox/Documents/Literature/MonteCarlo/Sarrut et al. - 2019 - Generative adversarial networks (GAN) for compact -MonteCarlo.pdf:pdf},
issn = {1361-6560},
journal = {Physics in Medicine & Biology},
keywords = {generative adversarial network,linac,monte-carlo simulation,phase-space},
month = {oct},
number = {21},
pages = {215004},
title = {{Generative adversarial networks (GAN) for compact beam source modelling in Monte Carlo simulations}},
url = {https://iopscience.iop.org/article/10.1088/1361-6560/ab3fc1},
volume = {64},
year = {2019}
}
@article{Sarrut2018,
abstract = {A method to speed up Monte-Carlo simulations of single photon emission computed tomography (SPECT) imaging is proposed. It uses an artificial neural network (ANN) to learn the angular response function (ARF) of a collimatordetector system. The ANN is trained once from a complete simulation including the complete detector head with collimator, crystal, and digitization process. In the simulation, particle tracking inside the SPECT head is replaced by a plane. Photons are stopped at the plane and the energy and direction are used as input to the ANN, which provides detection probabilities in each energy window. Compared to histogram-based ARF, the proposed method is less dependent on the statistics of the training data, provides similar simulation efficiency, and requires less training data. The implementation is available within the GATE platform.},
author = {Sarrut, D. and Krah, N. and Badel, J. N. and L{\'{e}}tang, J. M.},
doi = {10.1088/1361-6560/aae331},
file = {:Users/retico/cernbox/Documents/Literature/MonteCarlo/Sarrut_2018_Phys._Med._Biol._63_205013.pdf:pdf},
issn = {13616560},
journal = {Physics in Medicine and Biology},
keywords = {Monte-Carlo simulation,SPECT imaging,neural network,variance reduction technique},
number = {20},
pmid = {30238925},
title = {{Learning SPECT detector angular response function with neural network for accelerating Monte-Carlo simulations}},
volume = {63},
year = {2018}
}


@article{Shlomi2021,
abstract = {Particle physics is a branch of science aiming at discovering the fundamental laws of matter and forces. Graph neural networks are trainable functions which operate on graphs-sets of elements and their pairwise relations-and are a central method within the broader field of geometric deep learning. They are very expressive and have demonstrated superior performance to other classical deep learning approaches in a variety of domains. The data in particle physics are often represented by sets and graphs and as such, graph neural networks offer key advantages. Here we review various applications of graph neural networks in particle physics, including different graph constructions, model architectures and learning objectives, as well as key open problems in particle physics for which graph neural networks are promising.},
author = {Shlomi, Jonathan and Battaglia, Peter and Vlimant, Jean-Roch},
doi = {10.1088/2632-2153/abbf9a},
journal = {Mach. Learn.: Sci. Technol},
keywords = {graph neural network,high energy physics,machine learning,review},
pages = {21001},
title = {{Graph neural networks in particle physics}},
url = {https://doi.org/10.1088/2632-2153/abbf9a},
volume = {2},
year = {2021}
}


@article{Scarselli2009,
abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IR m that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities. Abstract-Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function () that maps a graph and one of its nodes into an-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
author = {Scarselli, F and Gori, M and Tsoi, A Chung and Hagenbuchner, M and Monfardini, G ; F and Monfardini, G},
doi = {10.1109/TNN.2008.2005605},
file = {::},
journal = {This journal article is available at Research IEEE TRANSACTIONS ON NEURAL NETWORKS},
keywords = {Index Terms-Graphical domains,graph neural networks (GNNs),graph processing,recursive neural networks},
number = {1},
pages = {61--80},
title = {{The graph neural network model}},
volume = {20},
year = {2009}
}


@article{Ju2020,
abstract = {Jet clustering is traditionally an unsupervised learning task because there is no unique way to associate hadronic final states with the quark and gluon degrees of freedom that generated them. However, for uncolored particles like W, Z, and Higgs bosons, it is possible to approximately (though not exactly) associate final state hadrons to their ancestor. By labeling simulated final state hadrons as descending from an uncolored particle, it is possible to train a supervised learning method to create boson jets. Such a method would operate on individual particles and identify connections between particles originating from the same uncolored particle. Graph neural networks are well-suited for this purpose as they can act on unordered sets and naturally create strong connections between particles with the same label. These networks are used to train a supervised jet clustering algorithm. The kinematic properties of these graph jets better match the properties of simulated Lorentz-boosted W bosons. Furthermore, the graph jets contain more information for discriminating W jets from generic quark jets. This work marks the beginning of a new exploration in jet physics to use machine learning to optimize the construction of jets and not only the observables computed from jet constituents.},
archivePrefix = {arXiv},
arxivId = {2008.06064},
author = {Ju, Xiangyang and Nachman, Benjamin},
doi = {10.1103/PhysRevD.102.075014},
eprint = {2008.06064},
file = {::},
issn = {24700029},
journal = {Physical Review D},
keywords = {doi:10.1103/PhysRevD.102.075014 url:https://doi.org/10.1103/PhysRevD.102.075014},
month = {oct},
number = {7},
pages = {075014},
publisher = {American Physical Society},
title = {{Supervised jet clustering with graph neural networks for Lorentz boosted bosons}},
url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.102.075014},
volume = {102},
year = {2020}
}
